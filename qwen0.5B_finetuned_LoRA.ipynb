{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U datasets accelerate peft transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T00:45:19.685324Z","iopub.execute_input":"2025-12-14T00:45:19.686023Z","iopub.status.idle":"2025-12-14T00:46:47.199092Z","shell.execute_reply.started":"2025-12-14T00:45:19.685996Z","shell.execute_reply":"2025-12-14T00:46:47.198357Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nCollecting accelerate\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\nCollecting peft\n  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, accelerate, peft\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.9.0\n    Uninstalling accelerate-1.9.0:\n      Successfully uninstalled accelerate-1.9.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.16.0\n    Uninstalling peft-0.16.0:\n      Successfully uninstalled peft-0.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.18.0 pyarrow-22.0.0 tokenizers-0.22.1 transformers-4.57.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install -U transformers datasets accelerate peft\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:02:29.558849Z","iopub.execute_input":"2025-12-14T01:02:29.559139Z","iopub.status.idle":"2025-12-14T01:04:02.390202Z","shell.execute_reply.started":"2025-12-14T01:02:29.559115Z","shell.execute_reply":"2025-12-14T01:04:02.389280Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nCollecting accelerate\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\nCollecting peft\n  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, accelerate, peft\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.9.0\n    Uninstalling accelerate-1.9.0:\n      Successfully uninstalled accelerate-1.9.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.16.0\n    Uninstalling peft-0.16.0:\n      Successfully uninstalled peft-0.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.18.0 pyarrow-22.0.0 tokenizers-0.22.1 transformers-4.57.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:04:02.391862Z","iopub.execute_input":"2025-12-14T01:04:02.392097Z","iopub.status.idle":"2025-12-14T01:04:32.405053Z","shell.execute_reply.started":"2025-12-14T01:04:02.392071Z","shell.execute_reply":"2025-12-14T01:04:32.404269Z"}},"outputs":[{"name":"stderr","text":"2025-12-14 01:04:12.253300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765674252.444308      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765674252.496088      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"# 1. Load the model from HuggingFace\n\n<span style=\"font-size: 18px\">\nIn this example, I'll use Qwen/Qwen2.5-0.5B-Instruct for fine-tuning.\n</span>","metadata":{}},{"cell_type":"code","source":"model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, \n                                            torch_dtype=torch.bfloat16,\n                                            device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:04:37.486699Z","iopub.execute_input":"2025-12-14T01:04:37.487595Z","iopub.status.idle":"2025-12-14T01:04:41.251638Z","shell.execute_reply.started":"2025-12-14T01:04:37.487567Z","shell.execute_reply":"2025-12-14T01:04:41.251079Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12a98f1252424ebe8cab9eead4243b80"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6da9b6fd5db4fdb872080e4877989d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a570e4d1c74a430b87b259bb13e2026b"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:05:02.420641Z","iopub.execute_input":"2025-12-14T01:05:02.421205Z","iopub.status.idle":"2025-12-14T01:05:02.484300Z","shell.execute_reply.started":"2025-12-14T01:05:02.421174Z","shell.execute_reply":"2025-12-14T01:05:02.483736Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:05:03.999440Z","iopub.execute_input":"2025-12-14T01:05:03.999728Z","iopub.status.idle":"2025-12-14T01:05:04.005500Z","shell.execute_reply.started":"2025-12-14T01:05:03.999705Z","shell.execute_reply":"2025-12-14T01:05:04.004841Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"layer_types\": [\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\"\n  ],\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.57.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## 1.1 Model Architechture\n<span style=\"font-size: 18px\">\n\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\n### 1.1.1 Model Size & Depth:\n>**\"num_hidden_layers\": 24**\n\n>**\"hidden_size\": 896**\n\n<br>\nThis shows that the model has a medium depth and small size.\n<br>\n\n### 1.1.2 Grouped Query Attention:\nGrouped Query Attention (GQA) is an optimization technique for transformer models that balances computational efficiency and model performance. Inspired by the multi-head attention mechanism introduced in the seminal \"Attention Is All You Need\" paper, GQA addresses limitations of its predecessors: multi-head attention (MHA) and multi-query attention (MQA). Below is a detailed analysis of its architecture, benchmarks and tradeoffs.\n\n<strong>Core Architechture</strong>\n<br><br>\n<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250626165907521883/file.webp\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\nGQA divides query heads into G groups, each sharing a single key and value head. This contrasts with:\n\nMHA: Each query head has unique key/value heads (high accuracy, high memory cost).\nMQA: All query heads share one key/value head (lower memory cost, reduced accuracy).\n\nIn this model:\n<br>\n>**\"num_attention_heads\": 14**\n\n>**\"num_key_value_heads\": 2**\n\n<br>\nThis means that we have 14 Q heads and 2 KV heads:\n<br>\nGroup 1:\n  Q0, Q1, Q2, Q3, Q4, Q5, Q6 -> Use KV_0 \n<br>\nGroup 2:\n  Q7, Q8, Q9, Q10, Q11, Q12, Q13 -> Use KV_1\n<br><br>\n<strong>Math Formula:</strong>\n<br><br>\n\n$$\n\\begin{aligned}\n\\text{Attention}(Q_i, K_g, V_g) &= softmax\\left( \\frac{Q_i K_g^{T}}{\\sqrt{d_k}} \\right) V_g\n\\end{aligned}\n$$\n\n### 1.1.3 Rotary Postional Embedding (RoPE):\n\nRoPE represents a novel approach in encoding positional information. Traditional methods, either absolute or relative, come with their limitations. Absolute positional embeddings assign a unique vector to each position, which though straightforward, doesn’t scale well and fails to capture relative positions effectively. Relative embeddings, on the other hand, focus on the distance between tokens, enhancing the model’s understanding of token relationships but complicating the model architecture.\n\nRoPE ingeniously combines the strengths of both. It encodes positional information in a way that allows the model to understand both the absolute position of tokens and their relative distances. This is achieved through a rotational mechanism, where each position in the sequence is represented by a rotation in the embedding space. The elegance of RoPE lies in its simplicity and efficiency, enabling models to better grasp the nuances of language syntax and semantics.\n\n<strong>The Mechanism of Rotary Positional Embeddings\n</strong>\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9T_o7ZbLK4mOSKJb5WNxeg.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\n**RoPE introduces a novel concept**. Instead of adding a positional vector, it applies a rotation to the word vector. Imagine a two-dimensional word vector for “dog.” To encode its position in a sentence, RoPE rotates this vector. The angle of rotation (θ) is proportional to the word’s position in the sentence. For instance, the vector is rotated by θ for the first position, 2θ for the second, and so on. This approach has several benefits:\n\n**Stability of Vectors**: Adding tokens at the end of a sentence doesn’t affect the vectors for words at the beginning, facilitating efficient caching.\nPreservation of Relative Positions: If two words, say “pig” and “dog,” maintain the same relative distance in different contexts, their vectors are rotated by the same amount. This ensures that the angle, and consequently the dot product between these vectors, remains constant\n\n<strong>Matrix Formulation of RoPE</strong>\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*42x3x4KKqSIKoajakjGmLg.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n\n### 1.1.4 Swish-Gated Linear Unit (SwiGLU)\n\n<strong>What is swish?</strong>\n<br><br>\nSwish is a smooth, non-monotonic — function that does not consistently increase or decrease — activation function defined as :\n\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*tDJKko60ciqzXEKs99fC5g.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\nβ is a trainable parameter, but most implementations do not use it, setting β = 1 and simplifying the function to : swish(x) = x * sigmoid(x) which is equivalent to the Sigmoid Linear Unit or SiLU.\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9ZNwhMAa9Ci36xOmEuMiOg.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\n\nSwish has been shown to outperform ReLU in many applications. Its main advantage is that it provides a smoother transition around 0, which leads to better optimization and faster convergence.\n<br><br>\n<strong>What is Gated Linear Unit:</strong><br><br>\nGated Linear Units (GLU) are neural network layers proposed by researchers at Microsoft in 2016. The idea behind this function is that it takes the output of a linear transformation and splits it into two parts: one part is passed through another linear transformation, while the second is passed through a sigmoid activation function. This is illustrated in the following formula:\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*9EIG-EIX837FKTM0\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\n**“The output of each layer is a linear projection x∗ W + b modulated by the gates σ(x ∗ V + c). Similar to LSTMs, these gates multiply each element of the matrix x∗W+b and control the information passed on in the hierarchy.”**\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*9Ebd6nY2fvkjzoFdcvhE9g.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\n<strong>What is SwiGLU:</strong>\nAs we mentioned earlier, SwiGLU is a combination of both Swish and GLU. It is basically a GLU, but instead of using the sigmoid function, we use Swish with β = 1, as illustrated in the following formula:\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s6xTNRLICLmjJ2UwQahegw.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n\n### 1.1.5 Root Mean Square Layer Normalization (RMSNorm):\n<strong>What is Layer Norm:</strong>\n<br>\nLayer Norm formula is defined as:\n$$\ny = \\frac{x – \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n$$\n<br><br>\nThe small quantity 𝜖 prevents division by zero. Mean 𝜇 and variance 𝜎2 are computed from input data across the feature dimension.\n<br><br>\n<strong>What is RMSNorm:</strong><br><br>\nMost recent transformer models use RMS Norm instead of LayerNorm. The key difference is that RMS Norm only scales the input without shifting it. The mathematical formulation is:\n\n$$\n\\text{RMSNorm}(x) = \\gamma \\odot \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}}\n$$\n</span>\n\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Load the dataset for fine tuning\n<span style=\"font-size: 18px\">\nI'll use HuggingFaceH4/MATH for fine tuning.\n</span>","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n \nds = load_dataset(\"HuggingFaceH4/MATH\", \"default\", split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:05:15.228489Z","iopub.execute_input":"2025-12-14T01:05:15.229302Z","iopub.status.idle":"2025-12-14T01:05:17.803780Z","shell.execute_reply.started":"2025-12-14T01:05:15.229250Z","shell.execute_reply":"2025-12-14T01:05:17.803018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be46cb8571b842fc8e13443429333abb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/351k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09913f392f5343ebba421f5faa318a89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/240k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e7292528fd1408e9a6752f9537a9d71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c43ae91f21495bb32c81db056c7c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/546 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ae7848b76e4720a902a4af321d8584"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.bos_token_id = tokenizer.bos_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:12:28.224680Z","iopub.execute_input":"2025-12-14T01:12:28.225487Z","iopub.status.idle":"2025-12-14T01:12:28.896645Z","shell.execute_reply.started":"2025-12-14T01:12:28.225450Z","shell.execute_reply":"2025-12-14T01:12:28.895804Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def format_math_example(example):\n    prompt = f\"<|im_start|>user\\nSolve the following math problem step by step: {example['problem']}<|im_end|>\\n<|im_start|>assistant\\n{example['solution']}<|im_end|>\\n\"\n    return {\"text\": prompt}\n\nds = ds.map(format_math_example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:12:31.871273Z","iopub.execute_input":"2025-12-14T01:12:31.871805Z","iopub.status.idle":"2025-12-14T01:12:31.935769Z","shell.execute_reply.started":"2025-12-14T01:12:31.871781Z","shell.execute_reply":"2025-12-14T01:12:31.935018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56204eb52dfd42f8bcdad9809eb9a852"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"def tokenize_func(examples):\n    enc = tokenizer(\n    examples[\"text\"],\n    padding=\"max_length\",\n    truncation=True,\n    max_length=512\n    )\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n    return enc\n\ntokenized_ds = ds.map(tokenize_func, batched=True, remove_columns=ds.column_names)\ntokenized_ds = tokenized_ds.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:13:58.778468Z","iopub.execute_input":"2025-12-14T01:13:58.779125Z","iopub.status.idle":"2025-12-14T01:13:59.361902Z","shell.execute_reply.started":"2025-12-14T01:13:58.779097Z","shell.execute_reply":"2025-12-14T01:13:59.361346Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d19d26734bba4cf488828be7057d5dd0"}},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"# 3. Using LoRA for fine-tuning\n<span style=\"font-size: 18px\">\n\n## 3.1 What is Low Rank Adaption (LoRA)?\n<strong>Key Features of LoRA</strong>\n- **Parameter Efficiency**: It reduces the number of trainable parameters, leading to lower memory usage during fine-tuning and inference.\n- **Computational Efficiency**: It minimizes matrix operations, reducing the computational workload on GPUs/TPUs and speeding up the fine-tuning process.\n- **Preservation of Pre-Trained Knowledge**: The original pre-trained model remains unchanged making it easy to revert to the base model when needed.\n- **Scalability**: It can be applied to various transformer-based models like GPT, BERT and T5 making it versatile for different tasks.\n- **Faster Fine-Tuning**: By updating fewer parameters, it accelerates the fine-tuning process compared to traditional methods.\n\n<strong>Architecture of LoRA</strong>\nLoRA is used with transformer-based models which are common in NLP tasks. Let's see how it works:\n\n- **Pre-Trained Backbone**: We start with a large transformer model like GPT or BERT that has already been trained on a range of data.\n- **Low-Rank Adaptation Layers**: It adds small low-rank matrices to the model’s attention mechanism. These matrices are the only parts of the model that get updated during fine-tuning.\n- **Frozen Original Parameters**: The original weights of the model are kept frozen. This means we don’t modify the entire model, just the added low-rank matrices.\n- **Task-Specific Fine-Tuning**: We fine-tune the low-rank matrices for the specific task such as sentiment analysis or translation while the rest of the model stays the same.\n<br><br>\nThis approach helps us adapt large models to new tasks without changing the entire structure making it more efficient.\n<br><br>\n\n<strong>Working of LoRA</strong>\n\nLoRA modifies the traditional fine-tuning process by introducing low-rank matrices into specific layers of a neural network allowing the model to adapt to new tasks without changing the entire model. Let's see how LoRA works:\n\n1. **Decomposing the Weight Matrix**\nInstead of updating the entire weight matrix during fine-tuning, it approximates it using two smaller low-rank matrices A and B. The adapted weight matrix (W') is calculated as:\n\n$$\nW = W' + BA\n$$\nHere W is the original weight matrix and A and B are the low-rank matrices. This decomposition allows the model to make task-specific adjustments without the need to retrain the entire model, drastically reducing the computational load.\n\n2. **Training Only the LoRA Parameters**\nDuring the fine-tuning process, only the low-rank matrices A and B are updated while the original model weights W remain frozen. This minimizes the number of parameters that need to be adjusted making fine-tuning faster and more memory-efficient compared to traditional methods where all model weights are updated.\n\n3. **Inference with Adapted Weights**\nAfter fine-tuning, the adapted weight matrix W′ is used for inference. This helps the model to make predictions for specific tasks, fine-tuned with minimal computational resources. Since only the low-rank matrices are updated, it maintains efficiency even during inference.\n\nBy using LoRA, we can adapt large pre-trained models to new tasks quickly and efficiently without the computational burden of full model fine-tuning.\n</span>","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=16,\n    task_type=TaskType.CAUSAL_LM,\n    lora_dropout=0.1,\n    lora_alpha=32,\n    target_modules=['q_proj', 'v_proj']\n)\nif hasattr(model, \"peft_config\"):\n    model = model.unload()\nmodel = get_peft_model(model, lora_config)\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen-math-finetuned\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:14:02.862808Z","iopub.execute_input":"2025-12-14T01:14:02.863391Z","iopub.status.idle":"2025-12-14T01:14:02.999307Z","shell.execute_reply.started":"2025-12-14T01:14:02.863367Z","shell.execute_reply":"2025-12-14T01:14:02.998545Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds['train'],\n    eval_dataset=tokenized_ds['test'],\n    tokenizer=tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:14:04.647746Z","iopub.execute_input":"2025-12-14T01:14:04.648014Z","iopub.status.idle":"2025-12-14T01:14:04.660850Z","shell.execute_reply.started":"2025-12-14T01:14:04.647992Z","shell.execute_reply":"2025-12-14T01:14:04.660123Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/3511874148.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:14:06.859120Z","iopub.execute_input":"2025-12-14T01:14:06.859886Z","iopub.status.idle":"2025-12-14T01:34:26.597813Z","shell.execute_reply.started":"2025-12-14T01:14:06.859858Z","shell.execute_reply":"2025-12-14T01:34:26.597253Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [504/504 20:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.259600</td>\n      <td>0.283799</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.297300</td>\n      <td>0.275894</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.268200</td>\n      <td>0.275000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=504, training_loss=0.35457197423019104, metrics={'train_runtime': 1219.1347, 'train_samples_per_second': 1.651, 'train_steps_per_second': 0.413, 'total_flos': 2219905981218816.0, 'train_loss': 0.35457197423019104, 'epoch': 3.0})"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"trainer.save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:39:43.614658Z","iopub.execute_input":"2025-12-14T01:39:43.615571Z","iopub.status.idle":"2025-12-14T01:39:43.822560Z","shell.execute_reply.started":"2025-12-14T01:39:43.615537Z","shell.execute_reply":"2025-12-14T01:39:43.821781Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import math\n\neval_results = trainer.evaluate(eval_dataset=tokenized_ds[\"test\"])\neval_loss = eval_results[\"eval_loss\"]\n\nperplexity = math.exp(eval_loss)\nprint(f\"Perplexity: {perplexity}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:41:12.165734Z","iopub.execute_input":"2025-12-14T01:41:12.166358Z","iopub.status.idle":"2025-12-14T01:41:33.294699Z","shell.execute_reply.started":"2025-12-14T01:41:12.166329Z","shell.execute_reply":"2025-12-14T01:41:33.294049Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 00:20]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Perplexity: 1.316530761186102\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"./qwen-math-finetuned\")\n\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\nmodel = PeftModel.from_pretrained(base_model, \"./qwen-math-finetuned\")\n\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nprompt = \"\"\"Solve the integral of x^2 from 0 to 1.\nWrite ONLY the final answer as a fraction on a single line.\nDo NOT show intermediate steps.\"\"\"\n\nout = generator(prompt, max_new_tokens=1000, return_full_text=True)\n\ntext = out[0]['generated_text'].strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T02:03:53.378193Z","iopub.execute_input":"2025-12-14T02:03:53.378730Z","iopub.status.idle":"2025-12-14T02:04:03.929874Z","shell.execute_reply.started":"2025-12-14T02:03:53.378706Z","shell.execute_reply":"2025-12-14T02:04:03.929210Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Final answer: \\]\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"import re\n\ndef extract_and_format_final_answer(text):\n    pattern = r'\\\\boxed\\{(.*)\\}'\n    matches = re.findall(pattern, text)\n    \n    if not matches:\n        return \"\"\n        \n    final_content = matches[-1]\n    \n    # Format thành LaTeX display\n    formatted = f\"\\\\\\\\[\\\\\\\\boxed{{{final_content}}}]\"\n    \n    return formatted\n\nresult = extract_and_format_final_answer(text)\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T02:10:01.829988Z","iopub.execute_input":"2025-12-14T02:10:01.830704Z","iopub.status.idle":"2025-12-14T02:10:01.836242Z","shell.execute_reply.started":"2025-12-14T02:10:01.830674Z","shell.execute_reply":"2025-12-14T02:10:01.835522Z"}},"outputs":[{"name":"stdout","text":"Solve the integral of x^2 from 0 to 1.\nWrite ONLY the final answer as a fraction on a single line.\nDo NOT show intermediate steps. To solve the integral \\(\\int_0^1 x^2 \\, dx\\), we can use the power rule for integration, which states that \\(\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C\\) for \\(n \\neq -1\\). Here, we will integrate \\(x^2\\) directly.\n\nThe integral is:\n\\[\n\\int_0^1 x^2 \\, dx\n\\]\n\nWe apply the power rule by multiplying each term in the parentheses by its exponent and then integrating:\n\n\\[\n\\left[ \\frac{x^3}{3} \\right]_0^1\n\\]\n\nNext, we evaluate this at the upper limit (1) and subtract the value at the lower limit (0):\n\n\\[\n\\frac{1^3}{3} - \\frac{0^3}{3}\n\\]\n\nThis simplifies to:\n\n\\[\n\\frac{1}{3} - 0 = \\frac{1}{3}\n\\]\n\nTherefore, the final answer is:\n\n\\[\n\\boxed{\\frac{1}{3}}\n\\]\n","output_type":"stream"}],"execution_count":74}]}