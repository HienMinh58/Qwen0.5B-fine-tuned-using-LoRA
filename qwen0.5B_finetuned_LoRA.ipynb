{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q datasets accelerate peft transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Load the model from HuggingFace\n\n<span style=\"font-size: 18px\">\nIn this example, I'll use Qwen/Qwen2.5-0.5B-Instruct for fine-tuning.\n</span>","metadata":{}},{"cell_type":"code","source":"model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, \n                                            torch_dtype=torch.bfloat16,\n                                            device_map=\"auto\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:05:02.420641Z","iopub.execute_input":"2025-12-14T01:05:02.421205Z","iopub.status.idle":"2025-12-14T01:05:02.484300Z","shell.execute_reply.started":"2025-12-14T01:05:02.421174Z","shell.execute_reply":"2025-12-14T01:05:02.483736Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:05:03.999440Z","iopub.execute_input":"2025-12-14T01:05:03.999728Z","iopub.status.idle":"2025-12-14T01:05:04.005500Z","shell.execute_reply.started":"2025-12-14T01:05:03.999705Z","shell.execute_reply":"2025-12-14T01:05:04.004841Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"layer_types\": [\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\",\n    \"full_attention\"\n  ],\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"transformers_version\": \"4.57.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## 1.1 Model Architechture\n<span style=\"font-size: 18px\">\n\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\n### 1.1.1 Model Size & Depth:\n>**\"num_hidden_layers\": 24**\n\n>**\"hidden_size\": 896**\n\n<br>\nThis shows that the model has a medium depth and small size.\n<br>\n\n### 1.1.2 Grouped Query Attention:\nGrouped Query Attention (GQA) is an optimization technique for transformer models that balances computational efficiency and model performance. Inspired by the multi-head attention mechanism introduced in the seminal \"Attention Is All You Need\" paper, GQA addresses limitations of its predecessors: multi-head attention (MHA) and multi-query attention (MQA). Below is a detailed analysis of its architecture, benchmarks and tradeoffs.\n\n<strong>Core Architechture</strong>\n<br><br>\n<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250626165907521883/file.webp\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\nGQA divides query heads into G groups, each sharing a single key and value head. This contrasts with:\n\nMHA: Each query head has unique key/value heads (high accuracy, high memory cost).\nMQA: All query heads share one key/value head (lower memory cost, reduced accuracy).\n\nIn this model:\n<br>\n>**\"num_attention_heads\": 14**\n\n>**\"num_key_value_heads\": 2**\n\n<br>\nThis means that we have 14 Q heads and 2 KV heads:\n<br>\nGroup 1:\n  Q0, Q1, Q2, Q3, Q4, Q5, Q6 -> Use KV_0 \n<br>\nGroup 2:\n  Q7, Q8, Q9, Q10, Q11, Q12, Q13 -> Use KV_1\n<br><br>\n<strong>Math Formula:</strong>\n<br><br>\n\n$$\n\\begin{aligned}\n\\text{Attention}(Q_i, K_g, V_g) &= softmax\\left( \\frac{Q_i K_g^{T}}{\\sqrt{d_k}} \\right) V_g\n\\end{aligned}\n$$\n\n### 1.1.3 Rotary Postional Embedding (RoPE):\n\nRoPE represents a novel approach in encoding positional information. Traditional methods, either absolute or relative, come with their limitations. Absolute positional embeddings assign a unique vector to each position, which though straightforward, doesn‚Äôt scale well and fails to capture relative positions effectively. Relative embeddings, on the other hand, focus on the distance between tokens, enhancing the model‚Äôs understanding of token relationships but complicating the model architecture.\n\nRoPE ingeniously combines the strengths of both. It encodes positional information in a way that allows the model to understand both the absolute position of tokens and their relative distances. This is achieved through a rotational mechanism, where each position in the sequence is represented by a rotation in the embedding space. The elegance of RoPE lies in its simplicity and efficiency, enabling models to better grasp the nuances of language syntax and semantics.\n\n<strong>The Mechanism of Rotary Positional Embeddings\n</strong>\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9T_o7ZbLK4mOSKJb5WNxeg.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\n**RoPE introduces a novel concept**. Instead of adding a positional vector, it applies a rotation to the word vector. Imagine a two-dimensional word vector for ‚Äúdog.‚Äù To encode its position in a sentence, RoPE rotates this vector. The angle of rotation (Œ∏) is proportional to the word‚Äôs position in the sentence. For instance, the vector is rotated by Œ∏ for the first position, 2Œ∏ for the second, and so on. This approach has several benefits:\n\n**Stability of Vectors**: Adding tokens at the end of a sentence doesn‚Äôt affect the vectors for words at the beginning, facilitating efficient caching.\nPreservation of Relative Positions: If two words, say ‚Äúpig‚Äù and ‚Äúdog,‚Äù maintain the same relative distance in different contexts, their vectors are rotated by the same amount. This ensures that the angle, and consequently the dot product between these vectors, remains constant\n\n<strong>Matrix Formulation of RoPE</strong>\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*42x3x4KKqSIKoajakjGmLg.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n\n### 1.1.4 Swish-Gated Linear Unit (SwiGLU)\n\n<strong>What is swish?</strong>\n<br><br>\nSwish is a smooth, non-monotonic ‚Äî function that does not consistently increase or decrease ‚Äî activation function defined as :\n\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*tDJKko60ciqzXEKs99fC5g.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\nŒ≤ is a trainable parameter, but most implementations do not use it, setting Œ≤ = 1 and simplifying the function to : swish(x) = x * sigmoid(x) which is equivalent to the Sigmoid Linear Unit or SiLU.\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9ZNwhMAa9Ci36xOmEuMiOg.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\n\nSwish has been shown to outperform ReLU in many applications. Its main advantage is that it provides a smoother transition around 0, which leads to better optimization and faster convergence.\n<br><br>\n<strong>What is Gated Linear Unit:</strong><br><br>\nGated Linear Units (GLU) are neural network layers proposed by researchers at Microsoft in 2016. The idea behind this function is that it takes the output of a linear transformation and splits it into two parts: one part is passed through another linear transformation, while the second is passed through a sigmoid activation function. This is illustrated in the following formula:\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*9EIG-EIX837FKTM0\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\n**‚ÄúThe output of each layer is a linear projection x‚àó W + b modulated by the gates œÉ(x ‚àó V + c). Similar to LSTMs, these gates multiply each element of the matrix x‚àóW+b and control the information passed on in the hierarchy.‚Äù**\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*9Ebd6nY2fvkjzoFdcvhE9g.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n<br><br>\n<strong>What is SwiGLU:</strong>\nAs we mentioned earlier, SwiGLU is a combination of both Swish and GLU. It is basically a GLU, but instead of using the sigmoid function, we use Swish with Œ≤ = 1, as illustrated in the following formula:\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s6xTNRLICLmjJ2UwQahegw.png\" style=\"display: block; margin: 0 auto;\" width=\"600px\">\n\n### 1.1.5 Root Mean Square Layer Normalization (RMSNorm):\n<strong>What is Layer Norm:</strong>\n<br>\nLayer Norm formula is defined as:\n$$\ny = \\frac{x ‚Äì \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n$$\n<br><br>\nThe small quantity ùúñ prevents division by zero. Mean ùúá and variance ùúé2 are computed from input data across the feature dimension.\n<br><br>\n<strong>What is RMSNorm:</strong><br><br>\nMost recent transformer models use RMS Norm instead of LayerNorm. The key difference is that RMS Norm only scales the input without shifting it. The mathematical formulation is:\n\n$$\n\\text{RMSNorm}(x) = \\gamma \\odot \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}}\n$$\n</span>\n\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Load the dataset for fine tuning\n<span style=\"font-size: 18px\">\nI'll use HuggingFaceH4/MATH for fine tuning.\n</span>","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n \nds = load_dataset(\"HuggingFaceH4/MATH\", \"default\", split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:05:15.228489Z","iopub.execute_input":"2025-12-14T01:05:15.229302Z","iopub.status.idle":"2025-12-14T01:05:17.803780Z","shell.execute_reply.started":"2025-12-14T01:05:15.229250Z","shell.execute_reply":"2025-12-14T01:05:17.803018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be46cb8571b842fc8e13443429333abb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/351k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09913f392f5343ebba421f5faa318a89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/240k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e7292528fd1408e9a6752f9537a9d71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c43ae91f21495bb32c81db056c7c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/546 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ae7848b76e4720a902a4af321d8584"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.bos_token_id = tokenizer.bos_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:12:28.224680Z","iopub.execute_input":"2025-12-14T01:12:28.225487Z","iopub.status.idle":"2025-12-14T01:12:28.896645Z","shell.execute_reply.started":"2025-12-14T01:12:28.225450Z","shell.execute_reply":"2025-12-14T01:12:28.895804Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def format_math_example(example):\n    prompt = f\"<|im_start|>user\\nSolve the following math problem step by step: {example['problem']}<|im_end|>\\n<|im_start|>assistant\\n{example['solution']}<|im_end|>\\n\"\n    return {\"text\": prompt}\n\nds = ds.map(format_math_example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:12:31.871273Z","iopub.execute_input":"2025-12-14T01:12:31.871805Z","iopub.status.idle":"2025-12-14T01:12:31.935769Z","shell.execute_reply.started":"2025-12-14T01:12:31.871781Z","shell.execute_reply":"2025-12-14T01:12:31.935018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56204eb52dfd42f8bcdad9809eb9a852"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"def tokenize_func(examples):\n    enc = tokenizer(\n    examples[\"text\"],\n    padding=\"max_length\",\n    truncation=True,\n    max_length=512\n    )\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n    return enc\n\ntokenized_ds = ds.map(tokenize_func, batched=True, remove_columns=ds.column_names)\ntokenized_ds = tokenized_ds.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:13:58.778468Z","iopub.execute_input":"2025-12-14T01:13:58.779125Z","iopub.status.idle":"2025-12-14T01:13:59.361902Z","shell.execute_reply.started":"2025-12-14T01:13:58.779097Z","shell.execute_reply":"2025-12-14T01:13:59.361346Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/746 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d19d26734bba4cf488828be7057d5dd0"}},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"# 3. Using LoRA for fine-tuning\n<span style=\"font-size: 18px\">\n\n## 3.1 What is Low Rank Adaption (LoRA)?\n<strong>Key Features of LoRA</strong>\n- **Parameter Efficiency**: It reduces the number of trainable parameters, leading to lower memory usage during fine-tuning and inference.\n- **Computational Efficiency**: It minimizes matrix operations, reducing the computational workload on GPUs/TPUs and speeding up the fine-tuning process.\n- **Preservation of Pre-Trained Knowledge**: The original pre-trained model remains unchanged making it easy to revert to the base model when needed.\n- **Scalability**: It can be applied to various transformer-based models like GPT, BERT and T5 making it versatile for different tasks.\n- **Faster Fine-Tuning**: By updating fewer parameters, it accelerates the fine-tuning process compared to traditional methods.\n\n<strong>Architecture of LoRA</strong>\nLoRA is used with transformer-based models which are common in NLP tasks. Let's see how it works:\n\n- **Pre-Trained Backbone**: We start with a large transformer model like GPT or BERT that has already been trained on a range of data.\n- **Low-Rank Adaptation Layers**: It adds small low-rank matrices to the model‚Äôs attention mechanism. These matrices are the only parts of the model that get updated during fine-tuning.\n- **Frozen Original Parameters**: The original weights of the model are kept frozen. This means we don‚Äôt modify the entire model, just the added low-rank matrices.\n- **Task-Specific Fine-Tuning**: We fine-tune the low-rank matrices for the specific task such as sentiment analysis or translation while the rest of the model stays the same.\n<br><br>\nThis approach helps us adapt large models to new tasks without changing the entire structure making it more efficient.\n<br><br>\n\n<strong>Working of LoRA</strong>\n\nLoRA modifies the traditional fine-tuning process by introducing low-rank matrices into specific layers of a neural network allowing the model to adapt to new tasks without changing the entire model. Let's see how LoRA works:\n\n1. **Decomposing the Weight Matrix**\nInstead of updating the entire weight matrix during fine-tuning, it approximates it using two smaller low-rank matrices A and B. The adapted weight matrix (W') is calculated as:\n\n$$\nW = W' + BA\n$$\nHere W is the original weight matrix and A and B are the low-rank matrices. This decomposition allows the model to make task-specific adjustments without the need to retrain the entire model, drastically reducing the computational load.\n\n2. **Training Only the LoRA Parameters**\nDuring the fine-tuning process, only the low-rank matrices A and B are updated while the original model weights W remain frozen. This minimizes the number of parameters that need to be adjusted making fine-tuning faster and more memory-efficient compared to traditional methods where all model weights are updated.\n\n3. **Inference with Adapted Weights**\nAfter fine-tuning, the adapted weight matrix W‚Ä≤ is used for inference. This helps the model to make predictions for specific tasks, fine-tuned with minimal computational resources. Since only the low-rank matrices are updated, it maintains efficiency even during inference.\n\nBy using LoRA, we can adapt large pre-trained models to new tasks quickly and efficiently without the computational burden of full model fine-tuning.\n</span>","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=16,\n    task_type=TaskType.CAUSAL_LM,\n    lora_dropout=0.1,\n    lora_alpha=32,\n    target_modules=['q_proj', 'v_proj']\n)\nif hasattr(model, \"peft_config\"):\n    model = model.unload()\nmodel = get_peft_model(model, lora_config)\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen-math-finetuned\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:14:02.862808Z","iopub.execute_input":"2025-12-14T01:14:02.863391Z","iopub.status.idle":"2025-12-14T01:14:02.999307Z","shell.execute_reply.started":"2025-12-14T01:14:02.863367Z","shell.execute_reply":"2025-12-14T01:14:02.998545Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds['train'],\n    eval_dataset=tokenized_ds['test'],\n    tokenizer=tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:14:04.647746Z","iopub.execute_input":"2025-12-14T01:14:04.648014Z","iopub.status.idle":"2025-12-14T01:14:04.660850Z","shell.execute_reply.started":"2025-12-14T01:14:04.647992Z","shell.execute_reply":"2025-12-14T01:14:04.660123Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/3511874148.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:14:06.859120Z","iopub.execute_input":"2025-12-14T01:14:06.859886Z","iopub.status.idle":"2025-12-14T01:34:26.597813Z","shell.execute_reply.started":"2025-12-14T01:14:06.859858Z","shell.execute_reply":"2025-12-14T01:34:26.597253Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [504/504 20:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.259600</td>\n      <td>0.283799</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.297300</td>\n      <td>0.275894</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.268200</td>\n      <td>0.275000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=504, training_loss=0.35457197423019104, metrics={'train_runtime': 1219.1347, 'train_samples_per_second': 1.651, 'train_steps_per_second': 0.413, 'total_flos': 2219905981218816.0, 'train_loss': 0.35457197423019104, 'epoch': 3.0})"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"trainer.save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:39:43.614658Z","iopub.execute_input":"2025-12-14T01:39:43.615571Z","iopub.status.idle":"2025-12-14T01:39:43.822560Z","shell.execute_reply.started":"2025-12-14T01:39:43.615537Z","shell.execute_reply":"2025-12-14T01:39:43.821781Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import math\n\neval_results = trainer.evaluate(eval_dataset=tokenized_ds[\"test\"])\neval_loss = eval_results[\"eval_loss\"]\n\nperplexity = math.exp(eval_loss)\nprint(f\"Perplexity: {perplexity}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T01:41:12.165734Z","iopub.execute_input":"2025-12-14T01:41:12.166358Z","iopub.status.idle":"2025-12-14T01:41:33.294699Z","shell.execute_reply.started":"2025-12-14T01:41:12.166329Z","shell.execute_reply":"2025-12-14T01:41:33.294049Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 00:20]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Perplexity: 1.316530761186102\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"./qwen-math-finetuned\")\n\nfrom peft import PeftModel\nbase_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\nmodel = PeftModel.from_pretrained(base_model, \"./qwen-math-finetuned\")\n\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nprompt = \"\"\"Solve the integral of x^2 from 0 to 1.\nWrite ONLY the final answer as a fraction on a single line.\nDo NOT show intermediate steps.\"\"\"\n\nout = generator(prompt, max_new_tokens=1000, return_full_text=True)\n\ntext = out[0]['generated_text'].strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T02:03:53.378193Z","iopub.execute_input":"2025-12-14T02:03:53.378730Z","iopub.status.idle":"2025-12-14T02:04:03.929874Z","shell.execute_reply.started":"2025-12-14T02:03:53.378706Z","shell.execute_reply":"2025-12-14T02:04:03.929210Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Final answer: \\]\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"import re\n\ndef extract_and_format_final_answer(text):\n    pattern = r'\\\\boxed\\{(.*)\\}'\n    matches = re.findall(pattern, text)\n    \n    if not matches:\n        return \"\"\n        \n    final_content = matches[-1]\n    \n    # Format th√†nh LaTeX display\n    formatted = f\"\\\\\\\\[\\\\\\\\boxed{{{final_content}}}]\"\n    \n    return formatted\n\nresult = extract_and_format_final_answer(text)\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T02:10:01.829988Z","iopub.execute_input":"2025-12-14T02:10:01.830704Z","iopub.status.idle":"2025-12-14T02:10:01.836242Z","shell.execute_reply.started":"2025-12-14T02:10:01.830674Z","shell.execute_reply":"2025-12-14T02:10:01.835522Z"}},"outputs":[{"name":"stdout","text":"Solve the integral of x^2 from 0 to 1.\nWrite ONLY the final answer as a fraction on a single line.\nDo NOT show intermediate steps. To solve the integral \\(\\int_0^1 x^2 \\, dx\\), we can use the power rule for integration, which states that \\(\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C\\) for \\(n \\neq -1\\). Here, we will integrate \\(x^2\\) directly.\n\nThe integral is:\n\\[\n\\int_0^1 x^2 \\, dx\n\\]\n\nWe apply the power rule by multiplying each term in the parentheses by its exponent and then integrating:\n\n\\[\n\\left[ \\frac{x^3}{3} \\right]_0^1\n\\]\n\nNext, we evaluate this at the upper limit (1) and subtract the value at the lower limit (0):\n\n\\[\n\\frac{1^3}{3} - \\frac{0^3}{3}\n\\]\n\nThis simplifies to:\n\n\\[\n\\frac{1}{3} - 0 = \\frac{1}{3}\n\\]\n\nTherefore, the final answer is:\n\n\\[\n\\boxed{\\frac{1}{3}}\n\\]\n","output_type":"stream"}],"execution_count":74}]}